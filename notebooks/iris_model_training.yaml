apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: iris-training-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"train-model": [{"key": "artifacts/$PIPELINERUN/train-model/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/outputs/mlpipeline_ui_metadata/data"},
      {"key": "artifacts/$PIPELINERUN/train-model/mlpipeline-metrics.tgz", "name":
      "mlpipeline-metrics", "path": "/tmp/outputs/mlpipeline_metrics/data"}]}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"train-model": [["mlpipeline-ui-metadata", "/tmp/outputs/mlpipeline_ui_metadata/data"],
      ["mlpipeline-metrics", "/tmp/outputs/mlpipeline_metrics/data"]], "upload-iris-data":
      []}'
    sidecar.istio.io/inject: "false"
    pipelines.kubeflow.org/pipeline_spec: '{"name": "iris-training-pipeline"}'
spec:
  pipelineSpec:
    tasks:
    - name: upload-iris-data
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            echo -n "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_iris_data():

                from sklearn import datasets
                import pandas as pd
                import numpy as np
                from minio import Minio
                from sklearn.model_selection import train_test_split
                from sklearn.ensemble import RandomForestClassifier

                def load_iris_data():
                    iris = datasets.load_iris()
                    data=pd.DataFrame({
                        'sepal length':iris.data[:,0],
                        'sepal width':iris.data[:,1],
                        'petal length':iris.data[:,2],
                        'petal width':iris.data[:,3],
                        'species':iris.target
                    })
                    data.head()
                    return data

                def load_minio():
                    minio_client = Minio(
                        "minio-service.mlops-demo-datascience.svc.cluster.local:9000",
                        access_key="minio",
                        secret_key="minio123",
                        secure=False
                    )
                    minio_bucket = "mlpipeline"

                    return minio_client, minio_bucket;

                # Get Data from Iris Data Set and push to Minio Storage
                iris_data = load_iris_data()
                minio_client, minio_bucket = load_minio()

                X=iris_data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features
                y=iris_data['species']  # Labels

                # Split dataset into training set and test set
                x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.01) # 70% training and 30% test
                load_minio()
                # save to numpy file, store in Minio
                np.save("/tmp/x_train.npy",x_train)
                minio_client.fput_object(minio_bucket,"x_train","/tmp/x_train.npy")

                np.save("/tmp/y_train.npy",y_train)
                minio_client.fput_object(minio_bucket,"y_train","/tmp/y_train.npy")

                np.save("/tmp/x_test.npy",x_test)
                minio_client.fput_object(minio_bucket,"x_test","/tmp/x_test.npy")

                np.save("/tmp/y_test.npy",y_test)
                minio_client.fput_object(minio_bucket,"y_test","/tmp/y_test.npy")

                print(f"x_train shape: {x_train.shape}")
                print(f"y_train shape: {y_train.shape}")

                print(f"x_test shape: {x_test.shape}")
                print(f"y_test shape: {y_test.shape}")

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload iris data', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_iris_data(**_parsed_args)
          image: image-registry.openshift-image-registry.svc:5000/mlops-demo-datascience/s2i-generic-data-science-notebook:v0.0.5
        metadata:
          annotations: {pipelines.kubeflow.org/component_spec: '{"implementation":
              {"container": {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
              -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "def upload_iris_data():\n\n    from sklearn import datasets\n    import
              pandas as pd\n    import numpy as np\n    from minio import Minio\n    from
              sklearn.model_selection import train_test_split\n    from sklearn.ensemble
              import RandomForestClassifier\n\n    def load_iris_data():\n        iris
              = datasets.load_iris()\n        data=pd.DataFrame({\n            ''sepal
              length'':iris.data[:,0],\n            ''sepal width'':iris.data[:,1],\n            ''petal
              length'':iris.data[:,2],\n            ''petal width'':iris.data[:,3],\n            ''species'':iris.target\n        })\n        data.head()\n        return
              data\n\n    def load_minio():\n        minio_client = Minio(\n            \"minio-service.mlops-demo-datascience.svc.cluster.local:9000\",\n            access_key=\"minio\",\n            secret_key=\"minio123\",\n            secure=False\n        )\n        minio_bucket
              = \"mlpipeline\"\n\n        return minio_client, minio_bucket;\n\n    #
              Get Data from Iris Data Set and push to Minio Storage\n    iris_data
              = load_iris_data()\n    minio_client, minio_bucket = load_minio()\n\n    X=iris_data[[''sepal
              length'', ''sepal width'', ''petal length'', ''petal width'']]  # Features\n    y=iris_data[''species'']  #
              Labels\n\n    # Split dataset into training set and test set\n    x_train,
              x_test, y_train, y_test = train_test_split(X, y, test_size=0.01) # 70%
              training and 30% test\n    load_minio()\n    # save to numpy file, store
              in Minio\n    np.save(\"/tmp/x_train.npy\",x_train)\n    minio_client.fput_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n\n    np.save(\"/tmp/y_train.npy\",y_train)\n    minio_client.fput_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n\n    np.save(\"/tmp/x_test.npy\",x_test)\n    minio_client.fput_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n\n    np.save(\"/tmp/y_test.npy\",y_test)\n    minio_client.fput_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n\n    print(f\"x_train
              shape: {x_train.shape}\")\n    print(f\"y_train shape: {y_train.shape}\")\n\n    print(f\"x_test
              shape: {x_test.shape}\")\n    print(f\"y_test shape: {y_test.shape}\")\n\nimport
              argparse\n_parser = argparse.ArgumentParser(prog=''Upload iris data'',
              description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
              = upload_iris_data(**_parsed_args)\n"], "image": "image-registry.openshift-image-registry.svc:5000/mlops-demo-datascience/s2i-generic-data-science-notebook:v0.0.5"}},
              "name": "Upload iris data"}'}
      timeout: 0s
    - name: train-model
      taskSpec:
        steps:
        - name: main
          args: ['----output-paths', /tmp/outputs/mlpipeline_ui_metadata/data, /tmp/outputs/mlpipeline_metrics/data]
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            echo -n "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def train_model():

                import numpy as np
                from minio import Minio
                from sklearn.ensemble import RandomForestClassifier
                import pickle
                from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
                import json

                def load_minio():
                    minio_client = Minio(
                        "minio-service.mlops-demo-datascience.svc.cluster.local:9000",
                        access_key="minio",
                        secret_key="minio123",
                        secure=False
                    )
                    minio_bucket = "mlpipeline"

                    return minio_client, minio_bucket;

                # Create Model and Train
                minio_client, minio_bucket = load_minio()
                minio_client.fget_object(minio_bucket,"x_train","/tmp/x_train.npy")
                x_train = np.load("/tmp/x_train.npy")

                minio_client.fget_object(minio_bucket,"y_train","/tmp/y_train.npy")
                y_train = np.load("/tmp/y_train.npy")

                minio_client.fget_object(minio_bucket,"x_test","/tmp/x_test.npy")
                x_test = np.load("/tmp/x_test.npy")

                minio_client.fget_object(minio_bucket,"y_test","/tmp/y_test.npy")
                y_test = np.load("/tmp/y_test.npy")

                #Create a Gaussian Classifier
                model=RandomForestClassifier(n_estimators=100)

                #Train the model using the training sets y_pred=clf.predict(X_test)
                model.fit(x_train,y_train)
                y_pred=model.predict(x_test)

                print(x_train)
                model.predict([[5,3,1.6,0.2]])

                filename = '/tmp/iris-model.pkl'
                pickle.dump(model, open(filename, 'wb'))

                # Upload model to minio
                minio_client.fput_object(minio_bucket, "iris-model", filename)

                # Output metrics

                confusion_matrix_metric = confusion_matrix(y_test, y_pred)
                accuracy_score_metric = accuracy_score(y_test, y_pred)
                classification_report_metric = classification_report(y_test,y_pred)

                print("ACCURACY IS: " + str(accuracy_score_metric))

                metrics = {
                  'metrics': [{
                      'name': 'model_accuracy',
                      'numberValue':  float(accuracy_score_metric),
                      'format' : "PERCENTAGE"
                    }]}
                metadata = {
                  'metadata': [{
                      'placeholder_key': 'placeholder_value'
                    }]}

                from collections import namedtuple
                output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])
                return output(json.dumps(metadata),json.dumps(metrics))

            import argparse
            _parser = argparse.ArgumentParser(prog='Train model', description='')
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = train_model(**_parsed_args)

            _output_serializers = [
                str,
                str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: image-registry.openshift-image-registry.svc:5000/mlops-demo-datascience/s2i-generic-data-science-notebook:v0.0.5
        stepTemplate:
          volumeMounts:
          - {name: mlpipeline-ui-metadata, mountPath: /tmp/outputs/mlpipeline_ui_metadata}
          - {name: mlpipeline-metrics, mountPath: /tmp/outputs/mlpipeline_metrics}
        volumes:
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        - name: mlpipeline-metrics
          emptyDir: {}
        metadata:
          annotations: {pipelines.kubeflow.org/component_spec: '{"implementation":
              {"container": {"args": ["----output-paths", {"outputPath": "mlpipeline_ui_metadata"},
              {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
              -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "def train_model():\n\n    import numpy as np\n    from minio import
              Minio\n    from sklearn.ensemble import RandomForestClassifier\n    import
              pickle\n    from sklearn.metrics import accuracy_score, confusion_matrix,
              classification_report\n    import json\n\n    def load_minio():\n        minio_client
              = Minio(\n            \"minio-service.mlops-demo-datascience.svc.cluster.local:9000\",\n            access_key=\"minio\",\n            secret_key=\"minio123\",\n            secure=False\n        )\n        minio_bucket
              = \"mlpipeline\"\n\n        return minio_client, minio_bucket;\n\n    #
              Create Model and Train\n    minio_client, minio_bucket = load_minio()\n    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n    x_train
              = np.load(\"/tmp/x_train.npy\")\n\n    minio_client.fget_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n    y_train
              = np.load(\"/tmp/y_train.npy\")\n\n    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n    x_test
              = np.load(\"/tmp/x_test.npy\")\n\n    minio_client.fget_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n    y_test
              = np.load(\"/tmp/y_test.npy\")\n\n    #Create a Gaussian Classifier\n    model=RandomForestClassifier(n_estimators=100)\n\n    #Train
              the model using the training sets y_pred=clf.predict(X_test)\n    model.fit(x_train,y_train)\n    y_pred=model.predict(x_test)\n\n    print(x_train)\n    model.predict([[5,3,1.6,0.2]])\n\n    filename
              = ''/tmp/iris-model.pkl''\n    pickle.dump(model, open(filename, ''wb''))\n\n    #
              Upload model to minio\n    minio_client.fput_object(minio_bucket, \"iris-model\",
              filename)\n\n    # Output metrics\n\n    confusion_matrix_metric = confusion_matrix(y_test,
              y_pred)\n    accuracy_score_metric = accuracy_score(y_test, y_pred)\n    classification_report_metric
              = classification_report(y_test,y_pred)\n\n    print(\"ACCURACY IS: \"
              + str(accuracy_score_metric))\n\n    metrics = {\n      ''metrics'':
              [{\n          ''name'': ''model_accuracy'',\n          ''numberValue'':  float(accuracy_score_metric),\n          ''format''
              : \"PERCENTAGE\"\n        }]}\n    metadata = {\n      ''metadata'':
              [{\n          ''placeholder_key'': ''placeholder_value''\n        }]}\n\n    from
              collections import namedtuple\n    output = namedtuple(''output'', [''mlpipeline_ui_metadata'',
              ''mlpipeline_metrics''])\n    return output(json.dumps(metadata),json.dumps(metrics))\n\nimport
              argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"----output-paths\",
              dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
              = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_model(**_parsed_args)\n\n_output_serializers
              = [\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
              OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
              "image": "image-registry.openshift-image-registry.svc:5000/mlops-demo-datascience/s2i-generic-data-science-notebook:v0.0.5"}},
              "name": "Train model", "outputs": [{"name": "mlpipeline_ui_metadata",
              "type": "UI_metadata"}, {"name": "mlpipeline_metrics", "type": "Metrics"}]}'}
      runAfter: [upload-iris-data]
      timeout: 0s
  timeout: 0s
